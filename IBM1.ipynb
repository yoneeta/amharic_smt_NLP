{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b864f090-2b43-4723-8384-29b0d23d0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using NLTK package as a toolkit for working with NLP in Python. \n",
    "# It provides various text processing libraries with a lot of test datasets.\n",
    "import nltk\n",
    "#nltk.download()\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "# Using the nltk.translate packages IBMModel1, Alignment and IBMModel1 for testing the translation between 2 languages corpus using IBM model 1.\n",
    "from nltk.translate import AlignedSent, Alignment, IBMModel1\n",
    "\n",
    "# Defaultdict is a sub-class of the dictionary class that returns a dictionary-like object.\n",
    "# Using it for for making higher dimensional dictionaries.\n",
    "from collections import defaultdict\n",
    "\n",
    "import json # Extracting data from the .json files\n",
    "\n",
    "source_language = 'ar' # The source language tag for Amharic language.\n",
    "translation_language = 'tr' # The translation language tag for English language.\n",
    "FILE = 'am_tr_pair_small.json'#data file with the translations\n",
    "iterations_num = 2 # Number of iterations to run the EM algorithm.\n",
    "\n",
    "englishPreposition=[\"a\",\"the\",\"is\",\"it\",\"be\",\"of\",\"off\",\"they\",\"has\",\"then\",\"be\",\"been\"]\n",
    "\n",
    "def fileOpener(FILEZ):\n",
    "    with open(FILEZ, 'r', encoding = 'utf-8') as f:\n",
    "        gcorpus = json.load(f)\n",
    "    return gcorpus\n",
    "\n",
    "def word_tag(corpus):\n",
    "    corpus_words = defaultdict(set)\n",
    "    for x in corpus:\n",
    "        for y in x:\n",
    "            words = x[y].split()\n",
    "            #print(words)\n",
    "            for word in words:\n",
    "                \n",
    "                #if word.lower() not in englishPreposition:\n",
    "                #print(word)\n",
    "                \n",
    "                corpus_words[y].add(word)\n",
    "    return corpus_words\n",
    "\n",
    "def ibm1_EM(word_tags):\n",
    "    \n",
    "    sentence_pair = word_tag(word_tags) # Variable containing the pair of sentences (source & translation) with tags. \n",
    "    s_total = {} # Initializing an array for s_total.\n",
    "    #print(sentence_pair)\n",
    "    # Sorce Language reading here.\n",
    "    for word in sentence_pair[source_language]:\n",
    "        s_total[word] = 0.0\n",
    "        #print(word)\n",
    "    \n",
    "    # Initialize t(e|f) uniformly.\n",
    "    uniformly_dict = defaultdict(dict) # Creating a variable for the dictionary.\n",
    "    for word_source in sentence_pair[source_language]: # for loop which runs over all words with tags as 'ar'.\n",
    "        for word_translation in sentence_pair[translation_language]: # for loop which runs over all words with tags as 'tr'.\n",
    "            #print(\"----\",word_source,word_translation,len(sentence_pair[source_language]))\n",
    "            if word_translation not in englishPreposition:\n",
    "                uniformly_dict[word_source][word_translation] = 1 / len(sentence_pair[source_language]) # The transition_probabilities to 1/(length of the sentence in the source language).\n",
    "    init_prob = uniformly_dict # The initial probability.\n",
    "    iterations = 1 # Starting the iteration as 0.\n",
    "    \n",
    "    \n",
    "    # While not converged do:\n",
    "    while iterations <= iterations_num:\n",
    "        current_prob = init_prob # A variable containing the initial probability.\n",
    "        count = defaultdict(dict) # Creating a count variable containing a dictionary for all e and f values.\n",
    "        total = defaultdict(dict) # Creating a total variable containing a dictionary for all f values.\n",
    "\n",
    "        # Initializing count:\n",
    "        # Nested for loop which runs over all words in both amharic and English languages and assigning their weight as 0.\n",
    "        for word_source in sentence_pair[source_language]:\n",
    "            for word_translation in sentence_pair[translation_language]:\n",
    "                count[word_source][word_translation] = 0 # Assigning the weight of all words as 0.\n",
    "\n",
    "        # Initializing total:\n",
    "        # for loop which runs over all words in English language and assigning their weight as 0.\n",
    "        for word in sentence_pair[translation_language]:\n",
    "            total[word] = 0 # Assigning the weight of all words in the Turkish language as 0.\n",
    "\n",
    "\n",
    "        # For all sentence pair (e|f) do:\n",
    "        # Compute normalization:\n",
    "        #print(\"Iteration \", \"(\", iterations, \")\")\n",
    "        for (es, fs) in [(pair[source_language].split(), pair[translation_language].split()) for pair in word_tags]:\n",
    "            #print(es,\"<<<>>>\",fs)\n",
    "            for e in es:  # For all words e in es do:\n",
    "                #print(\"stotal is\",s_total[e])\n",
    "                s_total[e] = 0 # Intitialize s_total as 0.\n",
    "                \n",
    "                for f in fs: # For all words f in fs do:\n",
    "                    #print(e,f,current_prob[e][f])\n",
    "                    if f not in englishPreposition:\n",
    "                        s_total[e] += current_prob[e][f] # Updating the initial probability.\n",
    "                    #print(\"----->>>>\",s_total[e])\n",
    "            #print(\"s-total(e):: \", \"ar:\",es, \"||\", \"tr:\", fs, \"==> \", s_total[e]) # Printing s-total(e).\n",
    "            \n",
    "                    \n",
    "\n",
    "            # Collect counts:\n",
    "            for e in es: # For all words e in e do: source language word\n",
    "                for f in fs: # For all words f in f do:\n",
    "                    if f not in englishPreposition:\n",
    "\n",
    "                        count[e][f] += (current_prob[e][f] / s_total[e])\n",
    "                        total[f] += current_prob[e][f] / s_total[e]\n",
    "\n",
    "\n",
    "        # Estimate probabilities:\n",
    "        for f in sentence_pair[translation_language]: # For all English words f do:\n",
    "            for e in sentence_pair[source_language]: # For all Amharic words e do:\n",
    "                if f not in englishPreposition:\n",
    "                    #print(\"target\",f,\"source\",e,current_prob[e][f],\"-\",count[e][f],\"-\",total[f])\n",
    "                    current_prob[e][f] = count[e][f] / total[f]\n",
    "                \n",
    "                \n",
    "        #print(\"--------------------------------------------------------------------------\")\n",
    "        \n",
    "        init_prob = current_prob # Updating the init_prob to the value of the current_prob.\n",
    "        iterations += 1 # increament for the number of iteration inside the while loop.\n",
    "\n",
    "    return current_prob # Returning the value of the current_prob.\n",
    "\n",
    "def maximum_prob(current_prob):\n",
    "    prob_result = {} # Initializing an array for prob_result.\n",
    "    \n",
    "    source_key = current_prob.keys() # .keys() method returns a view object that displays a list of all the keys in the dictionary in order of insertion.\n",
    "    translation_word = list(current_prob.values()) # A variable which stores the values of the dictionary inside a list.\n",
    "    translation_key = translation_word[0].keys() # A variable which extract the keys of the dictionary starting from position 0.\n",
    "\n",
    "    for word_source in source_key: # for loop which runs over all the keys titled as 'ar' Amharic language.\n",
    "        max_prob = 0.0 # Initializing the max_prob variable as 0.\n",
    "        max_translation_word = \"\" # A variable which stores the words of English language.\n",
    "        for word_translation in translation_key: # for loop which runs over all the keys titled as 'tr' English language.\n",
    "            if current_prob[word_source][word_translation] >= max_prob: # if condition which takes the pair of sentences with their current_prob and compare them with the max_prob.\n",
    "                max_prob = current_prob[word_source][word_translation] # if the current_prob is bigger or equal to the max_prob, then assign the pair of sentences with their current_prob as max_prob.\n",
    "                max_translation_word = word_translation  # A variable which stores the words of English language.\n",
    "        \n",
    "        prob_result[word_source] = max_translation_word # Storing the words in an array from English language with the most probability with their corresponding from Amharic language in the same position.\n",
    "\n",
    "    return prob_result # Returning the maximum probability.\n",
    "\n",
    "def alignment(prob_result, word_tags):\n",
    "    \n",
    "    num = 1\n",
    "    for x in word_tags: # for loop which runs over all the sentences.\n",
    "        num += 1\n",
    "        \n",
    "        # Nested for loop for the alignment between each Araibc word with it's corresponding English word.\n",
    "        i = 0\n",
    "        for word_source in x[source_language].split():\n",
    "            j = 0\n",
    "            for word_translation in x[translation_language].split():\n",
    "                if word_translation == prob_result[word_source]: # if condition to match each Amharic word with it's corresponding English word.\n",
    "                    alignment = str(i) + \"-\" + str(j) + \" \"\n",
    "                    print(alignment, end = \"\")\n",
    "                    j = j + 1\n",
    "                    break\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "        print(\"\\n\")\n",
    "\n",
    "def JsonSaver(jsonName,Content):\n",
    "    with codecs.open(jsonName, 'w', 'utf8') as f:\n",
    "        f.write(json.dumps(Content,indent=4, sort_keys = True, ensure_ascii=False))\n",
    "\n",
    "def amharic_english():\n",
    "\n",
    "    ##parsing the json file and storing it in an object\n",
    "    #with open(FILE, 'r', encoding = 'utf-8') as f:\n",
    "    #    corpus = json.load(f)\n",
    "    corpus=fileOpener(FILE)\n",
    "    \n",
    "    # Calling the ibm1_EM function with passing the value obtained by the word_tag function.\n",
    "    ibm_em_tr = ibm1_EM(corpus)\n",
    "    \n",
    "    # Calling the maximum_prob function with passing the value obtained by the ibm1_EM function.\n",
    "    alignment_tr = maximum_prob(ibm_em_tr)\n",
    "    \n",
    "    \n",
    "    JsonSaver(\"AbebeBesoBela max probability.json\",alignment_tr)\n",
    "    JsonSaver(\"AbebeBesoBela statistics overall.json\",ibm_em_tr)\n",
    "    # Calling the alignment function with passing the values obtained by the maximum_prob function and the object containing sentences with tags from word_tag function.\n",
    "    alignment(alignment_tr, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16032f",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e453cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-2 1-2 2-2 3-2 4-2 5-2 \n",
      "\n",
      "1-11 2-11 3-11 4-11 5-11 6-5 7-11 8-11 \n",
      "\n",
      "0-2 1-2 2-2 3-2 4-2 5-2 \n",
      "\n",
      "0-2 1-1 2-1 3-0 4-1 5-1 6-1 7-1 \n",
      "\n",
      "0-4 1-2 2-2 3-2 4-2 5-2 \n",
      "\n",
      "0-0 1-2 3-2 4-2 5-2 \n",
      "\n",
      "0-2 1-2 2-2 3-2 \n",
      "\n",
      "0-0 1-0 2-1 \n",
      "\n",
      "0-0 1-0 2-1 \n",
      "\n",
      "0-0 1-2 2-2 \n",
      "\n",
      "0-0 1-1 2-1 \n",
      "\n",
      "0-0 1-2 2-1 \n",
      "\n",
      "0-0 1-0 2-1 \n",
      "\n",
      "0-0 1-2 2-1 \n",
      "\n",
      "0-0 1-2 2-1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    amharic_english() #Translation.\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22bd31-b0eb-4b1c-95df-de2ae1d4de03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e7a93-891a-4c4a-9f46-d68a3189e5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
